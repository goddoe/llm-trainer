#!/usr/bin/env python3
"""
ê¸°ê³„ì ìœ¼ë¡œ ì¶”ì¶œí•œ ìŠ¤í‚¤ë§ˆë¡œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
"""

import json
import subprocess
import sys
import glob
import csv
import random
from pathlib import Path

def get_sample_texts(num_samples=10):
    """CSVì—ì„œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    texts = []
    csv_files = glob.glob("./data/**/*.csv", recursive=True)[:5]

    for csv_file in csv_files:
        try:
            with open(csv_file, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    sentence = row.get('sentence', '').strip()
                    if sentence and len(sentence) > 10:
                        texts.append(sentence)
                        if len(texts) >= num_samples:
                            return texts
        except Exception as e:
            continue

    return texts

def test_huggingface_inference():
    """HuggingFace ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("HuggingFace Inference Test with Extracted Schema")
    print("="*60)

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    texts = get_sample_texts(5)

    if not texts:
        print("âŒ No sample texts found")
        return False

    results = []

    for i, text in enumerate(texts, 1):
        print(f"\n[Test {i}]")
        print(f"Input: {text[:100]}...")

        cmd = [
            "uv", "run", "python", "src/inference_structured.py",
            "--model-path", "./outputs/quick_test/merged",
            "--text", text,
            "--schema-file", "data/schemas/extracted_entities_schema.json",
            "--temperature", "0.1",
            "--max-new-tokens", "200"
        ]

        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)

            # ê²°ê³¼ íŒŒì‹±
            output_lines = result.stdout.strip().split('\n')
            for line in output_lines:
                if 'Output:' in line:
                    json_start = line.find('{')
                    if json_start != -1:
                        try:
                            output_json = json.loads(line[json_start:])
                            print(f"âœ… Success: {json.dumps(output_json, ensure_ascii=False)}")
                            results.append(True)
                        except json.JSONDecodeError:
                            print(f"âš ï¸ Invalid JSON: {line[json_start:json_start+50]}...")
                            results.append(False)
                    break

        except subprocess.TimeoutExpired:
            print("âŒ Timeout")
            results.append(False)
        except Exception as e:
            print(f"âŒ Error: {e}")
            results.append(False)

    success_rate = sum(results) / len(results) * 100 if results else 0
    print(f"\nğŸ“Š Success Rate: {success_rate:.1f}% ({sum(results)}/{len(results)})")

    return success_rate > 0

def test_vllm_inference():
    """vLLM ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("vLLM Inference Test with Extracted Schema")
    print("="*60)

    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    texts = get_sample_texts(3)

    if not texts:
        print("âŒ No sample texts found")
        return False

    # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    with open('test_texts_extracted.txt', 'w', encoding='utf-8') as f:
        for text in texts:
            f.write(text + '\n')

    cmd = [
        "uv", "run", "python", "src/inference_vllm_structured.py",
        "--model-path", "./outputs/quick_test/merged",
        "--input-file", "test_texts_extracted.txt",
        "--output-file", "test_results_extracted.json",
        "--schema-file", "data/schemas/gold_answer_schema.json",
        "--batch-size", "3",
        "--max-tokens", "8192"
    ]

    print("Running vLLM batch inference...")

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)

        if result.returncode == 0 and Path("test_results_extracted.json").exists():
            with open("test_results_extracted.json", 'r', encoding='utf-8') as f:
                results = json.load(f)

            print(f"\nâœ… Processed {len(results)} texts")

            for i, item in enumerate(results, 1):
                print(f"\n[Result {i}]")
                print(f"Input: {item['input'][:80]}...")
                if isinstance(item['output'], dict):
                    print(f"Output: {json.dumps(item['output'], ensure_ascii=False, indent=2)}")
                else:
                    print(f"Output: {item['output']}")

            return True
        else:
            print(f"âŒ vLLM failed: {result.stderr[:200]}")
            return False

    except subprocess.TimeoutExpired:
        print("âŒ Timeout (120s)")
        return False
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def analyze_schema():
    """ìŠ¤í‚¤ë§ˆ ë¶„ì„ ë° í†µê³„"""
    print("\n" + "="*60)
    print("Schema Analysis")
    print("="*60)

    # ìŠ¤í‚¤ë§ˆ ë¡œë“œ - gold_answer_schema ì‚¬ìš©
    schema_file = "data/schemas/gold_answer_schema.json" if Path("data/schemas/gold_answer_schema.json").exists() else "data/schemas/extracted_entities_schema.json"
    with open(schema_file, 'r', encoding='utf-8') as f:
        schema = json.load(f)

    # ë¶„ì„ ê²°ê³¼ ë¡œë“œ - gold_answer_analysis ì‚¬ìš©
    analysis_file = "data/schemas/gold_answer_analysis.json" if Path("data/schemas/gold_answer_analysis.json").exists() else "data/schemas/extracted_entities_analysis.json"
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis = json.load(f)

    # Coverage ì •ë³´ í™•ì¸
    if 'sentences_with_entities' in analysis:
        print(f"\nğŸ“Š Coverage: {analysis['sentences_with_entities']}/{analysis['total_sentences']} sentences ({analysis['sentences_with_entities']/analysis['total_sentences']*100:.1f}%)")
    elif 'rows_with_data' in analysis:
        print(f"\nğŸ“Š Coverage: {analysis['rows_with_data']}/{analysis['total_rows']} rows ({analysis['rows_with_data']/analysis['total_rows']*100:.1f}%)")

    print(f"\nğŸ”‘ Entity Types: {len(schema)}")

    extract_types = [s for s in schema if s['type'] == 'extract']
    enum_types = [s for s in schema if s['type'] == 'enum']

    print(f"  - Extract types: {len(extract_types)}")
    print(f"  - Enum types: {len(enum_types)}")

    print("\nğŸ“ˆ Top Entities by Frequency:")

    # gold_answer_analysis í˜•ì‹ í™•ì¸
    if 'key_details' in analysis:
        # gold_answer_analysis í˜•ì‹
        for key in ['í™˜ì ì¦ìƒ1', 'í™˜ìë°œìƒ ìœ í˜•_ì§ˆë³‘_ë³‘ë ¥', 'ì˜ì‹ìƒíƒœ_1ì°¨', 'í™œë ¥ì§•í›„_1ì°¨_ë§¥ë°•', 'ì£¼ì¦ìƒ(1ê°œì„ íƒ)']:
            if key in analysis['key_details']:
                entity_data = analysis['key_details'][key]
                freq = entity_data['frequency']
                unique = entity_data['unique_values']
                print(f"  {key}: {freq}íšŒ, {unique}ê°œ ê³ ìœ ê°’")
    elif 'entity_types' in analysis:
        # extracted_entities_analysis í˜•ì‹
        for entity_name in ['ì‹ ì²´ë¶€ìœ„', 'ì¦ìƒ', 'ì¥ì†Œ', 'ì‹œê°„', 'ì¸ë¬¼']:
            if entity_name in analysis['entity_types']:
                entity_data = analysis['entity_types'][entity_name]
                top_3 = entity_data['top_10'][:3]
                top_3_str = ', '.join([f"{v[0]}({v[1]})" for v in top_3])
                print(f"  {entity_name}: {top_3_str}")

    return True

def validate_extraction():
    """ì¶”ì¶œëœ ì—”í‹°í‹° ê²€ì¦"""
    print("\n" + "="*60)
    print("Extraction Validation")
    print("="*60)

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_sentences = [
        "ë¨¸ë¦¬ê°€ ì•„í”„ê³  ì—´ì´ ë‚˜ìš”",
        "ì–´ì œë¶€í„° ë°°ê°€ ì•„íŒŒì„œ ë³‘ì›ì— ê°”ì–´ìš”",
        "í• ë¨¸ë‹ˆê°€ ê³„ë‹¨ì—ì„œ ë„˜ì–´ì¡Œì–´ìš”",
        "ì§€ê¸ˆ ì˜ì‹ì´ ìˆë‚˜ìš”?",
        "ì‚°ì†Œë§ˆìŠ¤í¬ í•„ìš”í•´ìš”"
    ]

    # ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ import
    import sys
    sys.path.append('scripts')
    from extract_entities_from_csv import extract_entities_from_text

    print("\nğŸ” Testing entity extraction on sample sentences:")

    for sentence in test_sentences:
        entities = extract_entities_from_text(sentence)
        print(f"\nText: {sentence}")
        print(f"Extracted: {json.dumps(entities, ensure_ascii=False, indent=2)}")

    return True

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("="*60)
    print("ğŸ§ª EXTRACTED SCHEMA TEST SUITE")
    print("="*60)

    # í•„ìš” íŒŒì¼ í™•ì¸
    required_files = [
        "data/schemas/extracted_entities_schema.json",
        "data/schemas/extracted_entities_analysis.json"
    ]

    for file in required_files:
        if not Path(file).exists():
            print(f"âŒ Missing required file: {file}")
            print("Please run: uv run python scripts/extract_entities_from_csv.py")
            return 1

    results = []

    # 1. ìŠ¤í‚¤ë§ˆ ë¶„ì„
    print("\n[1/4] Analyzing Schema...")
    results.append(analyze_schema())

    # 2. ì¶”ì¶œ ê²€ì¦
    print("\n[2/4] Validating Extraction...")
    results.append(validate_extraction())

    # 3. HuggingFace í…ŒìŠ¤íŠ¸
    print("\n[3/4] Testing HuggingFace Inference...")
    results.append(test_huggingface_inference())

    # 4. vLLM í…ŒìŠ¤íŠ¸ (ì„ íƒì )
    if "--with-vllm" in sys.argv:
        print("\n[4/4] Testing vLLM Inference...")
        results.append(test_vllm_inference())
    else:
        print("\n[4/4] Skipping vLLM test (use --with-vllm to enable)")

    # ìµœì¢… ê²°ê³¼
    print("\n" + "="*60)
    print("ğŸ“Š FINAL RESULTS")
    print("="*60)

    test_names = ["Schema Analysis", "Extraction Validation", "HuggingFace Test", "vLLM Test"]
    for i, (name, result) in enumerate(zip(test_names[:len(results)], results)):
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {name}: {status}")

    success_rate = sum(results) / len(results) * 100
    print(f"\nğŸ¯ Overall Success Rate: {success_rate:.1f}%")

    if success_rate < 50:
        print("\nğŸ’¡ Tip: The model needs more training. Try:")
        print("  uv run python -m src.train --config configs/train_config.yaml --num-epochs 3")

    return 0 if success_rate >= 50 else 1

if __name__ == "__main__":
    sys.exit(main())